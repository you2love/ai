<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习 - AI技术学习平台</title>
    <link rel="stylesheet" href="css/styles.css">
    <!-- Prism.js syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
</head>
<body>
    <section class="section">
        <div class="container">
            <h2 class="section-title">🕸️ 深度学习</h2>
            <p class="section-subtitle">基于神经网络的机器学习方法</p>

            <!-- 什么是深度学习 -->
            <div class="card" style="margin-bottom: 2rem;">
                <div class="card-header">
                    <div class="card-icon">🧠</div>
                    <div>
                        <div class="card-title">什么是深度学习？</div>
                        <div class="card-subtitle">深度学习 vs 机器学习</div>
                    </div>
                </div>
                <div class="card-body">
                    <p><strong>深度学习</strong>是机器学习的一个子领域，使用多层神经网络来学习数据的复杂模式。</p>

                    <div class="comparison-row" style="margin-top: 1rem;">
                        <div class="comparison-cell" style="font-weight: 600;">对比</div>
                        <div class="comparison-cell" style="background: #fef2f2;">传统机器学习</div>
                        <div class="comparison-cell" style="background: #ecfdf5;">深度学习</div>
                    </div>
                    <div class="comparison-row">
                        <div class="comparison-cell">特征提取</div>
                        <div class="comparison-cell">人工设计</div>
                        <div class="comparison-cell">自动学习</div>
                    </div>
                    <div class="comparison-row">
                        <div class="comparison-cell">数据需求</div>
                        <div class="comparison-cell">中小规模</div>
                        <div class="comparison-cell">大规模</div>
                    </div>
                    <div class="comparison-row">
                        <div class="comparison-cell">计算需求</div>
                        <div class="comparison-cell">CPU即可</div>
                        <div class="comparison-cell">需要GPU</div>
                    </div>
                    <div class="comparison-row">
                        <div class="comparison-cell">可解释性</div>
                        <div class="comparison-cell">较好</div>
                        <div class="comparison-cell">较差</div>
                    </div>
                </div>
            </div>

            <!-- 神经网络基础 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">🧠 神经网络基础</h3>

            <div class="card">
                <div class="card-header">
                    <div class="card-icon">🔬</div>
                    <div>
                        <div class="card-title">神经元结构</div>
                    </div>
                </div>
                <div class="card-body">
                    <div class="diagram-box">
                        <div class="diagram-title">单个神经元的工作原理</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│     输入 x₁ ──权重 w₁──┐                                        │
│                       │                                        │
│     输入 x₂ ──权重 w₂──┼──▶ 求和 Σ(w·x + b) ──▶ 激活函数 ──▶ 输出 │
│                       │                                        │
│     输入 x₃ ──权重 w₃──┘                                        │
│                                                                 │
│  偏置 b: 调整神经元激活的阈值                                    │
│  激活函数: 引入非线性（ReLU、Sigmoid、Tanh）                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">多层神经网络</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   输入层          隐藏层1          隐藏层2          输出层      │
│                                                                 │
│     ○─┐        ┌───┐            ┌───┐            ┌─┐          │
│     ○─┼──────▶│ ○ │──────────▶│ ○ │──────────▶│○│          │
│     ○─┤        └───┘            └───┘            └─┘          │
│     ○─┤        ┌───┐            ┌───┐                         │
│     ○─┼──────▶│ ○ │──────────▶│ ○ │                         │
│     ○─┘        └───┘            └───┘                         │
│                                                                 │
│   [x₁,x₂,    →  特征提取    →  特征组合   →  最终预测         │
│    x₃,x₄]                                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>
                </div>
            </div>

            <!-- CNN 详解 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">🖼️ CNN 卷积神经网络</h3>

            <div class="card">
                <div class="card-header">
                    <div class="card-icon">🖼️</div>
                    <div>
                        <div class="card-title">CNN - 擅长处理图像</div>
                        <div class="card-subtitle">卷积层 + 池化层 + 全连接层</div>
                    </div>
                </div>
                <div class="card-body">
                    <div class="diagram-box">
                        <div class="diagram-title">CNN 处理图像流程</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  输入图像              卷积层              池化层           全连接层       │
│  (224×224×3)         提取特征           压缩尺寸          分类输出        │
│                                                                             │
│  ┌─────────┐       ┌─────────┐       ┌─────────┐       ┌─────────┐       │
│  │ ████    │       │ ░▓░▓░   │       │ ░▓░     │       │         │       │
│  │ ████    │  ──▶  │ ░▓░▓░   │  ──▶  │ ░▓░     │  ──▶  │    ○    │       │
│  │ ████    │       │ ░▓░▓░   │       │         │       │    ○    │       │
│  │ ████    │       │         │       │         │       │    ○    │       │
│  └─────────┘       └─────────┘       └─────────┘       │  Softmax│       │
│      │                   │                   │         └─────────┘       │
│      │                   │                   │              │            │
│   原始图像           边缘检测          特征压缩         [猫,狗,车...]      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">卷积操作原理</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   输入特征图                    卷积核 (3×3)          输出特征图         │
│                                                                         │
│   ┌───┬───┬───┬───┐        ┌───┬───┬───┐        ┌───┬───┬───┐        │
│   │ 1 │ 2 │ 3 │ 4 │        │ 1 │ 0 │ 1 │        ┌───┬───┬───┐        │
│   ├───┼───┼───┼───┤        ├───┼───┼───┤        │ 4 │ 6 │ 8 │        │
│   │ 5 │ 6 │ 7 │ 8 │   ✕    │ 0 │ 1 │ 0 │   =    ├───┼───┼───┤        │
│   ├───┼───┼───┼───┤        ├───┼───┼───┤        │ 6 │ 8 │10 │        │
│   │ 9 │10 │11 │12 │        │ 1 │ 0 │ 1 │        └───┴───┴───┘        │
│   └───┴───┴───┴───┘        └───┴───┴───┘                              │
│                                                                         │
│   滑动窗口计算：                                                         │
│   (1×1 + 2×0 + 3×1) + (5×1 + 6×0 + 7×1) + (9×1 + 10×0 + 11×1) = 4      │
│                                                                         │
│   ✓ 权值共享：同一个卷积核共享权重                                       │
│   ✓ 局部连接：只关注局部区域                                            │
│   ✓ 平移不变：检测相同特征                                               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">经典CNN架构演进</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│  LeNet-5        AlexNet          VGG-16        ResNet-50      EfficientNet │
│  (1998)        (2012)           (2014)         (2015)          (2019)    │
│                                                                         │
│   5层           8层             16层           50层            82层      │
│                                                                         │
│   简单          深度突破        更深更宽        残差连接        效率优化  │
│   手写数字      ImageNet冠军    统一结构        解决梯度消失    参数更少  │
│                                                                         │
│  ┌───┐        ┌───────┐       ┌───────┐      ┌────────┐      ┌─────┐   │
│  │输入│        │输入   │       │输入   │      │输入    │      │输入 │   │
│  └───┘        └───────┘       └───────┘      └────────┘      └─────┘   │
│     │            │              │                 │              │       │
│  ┌──┴──┐      ┌──┴──┐       ┌──┴──┐      ┌───┐ ┌──┴──┐      ┌───┴───┐  │
│  │Conv │      │Conv  │       │Conv  │      │Conv││Conv │      │Conv   │  │
│  │ Pool│      │Pool  │×5     │Pool  │×5    │   ││Skip │      │MBConv │×20 │
│  └──┬──┘      └──┬──┘       └──┬──┘      │   │└─────┘      └───┬───┘  │
│     │            │              │        ┌┴──┴┐                │       │
│   ┌──┴──┐      ┌──┴──┐       ┌──┴──┐    │Sum │              ┌─┴─┐     │
│  │FC   │      │FC×3  │       │FC×3  │    └─────┘             │FC │     │
│  │Output│      │Output│       │Output│          │            │Out│     │
│  └─────┘      └──────┘       └──────┘      └────────┘        └─────┘    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="info-box info">
                        <div class="info-box-title">💡 CNN核心特点</div>
                        <p>✓ <strong>卷积层</strong>：用小窗口提取局部特征（边缘、纹理、形状）</p>
                        <p>✓ <strong>池化层</strong>：压缩特征图，减少计算量（最大池化、平均池化）</p>
                        <p>✓ <strong>权值共享</strong>：同一卷积核在整个图像上共享，大幅减少参数</p>
                        <p>✓ <strong>平移不变</strong>：物体位置变化不影响识别</p>
                    </div>
                </div>
            </div>

            <!-- RNN/LSTM 详解 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">📝 RNN/LSTM 循环神经网络</h3>

            <div class="card">
                <div class="card-header">
                    <div class="card-icon">📝</div>
                    <div>
                        <div class="card-title">RNN/LSTM - 擅长处理序列</div>
                        <div class="card-subtitle">有记忆的神经网络</div>
                    </div>
                </div>
                <div class="card-body">
                    <div class="diagram-box">
                        <div class="diagram-title">RNN 循环结构</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   折叠形式                          展开形式                            │
│                                                                         │
│         ┌─────────┐                      t-1        t         t+1      │
│         │         │                    ┌───┐     ┌───┐     ┌───┐      │
│         │    ●    │◀────h───────────▶│ ●  │───▶│ ●  │───▶│ ●  │      │
│         │         │                   └───┘     └───┘     └───┘      │
│      h  │         │                     │         │         │         │
│    ────▶│         │──▶ y                │         │         │         │
│         └─────────┘              x_t-1   │    x_t  │    x_t+1│         │
│                                     └─────┴─────────┴─────────┘         │
│   输入x ──▶ [处理] ──▶ 输出y           时间步按顺序处理                  │
│   状态h ──▶ [传递] ──▶ 记忆                                           │
│                                                                         │
│   核心思想：每一时刻的输出取决于当前输入 + 之前的所有记忆              │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">RNN 计算公式</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   h_t = tanh(W_xh · x_t + W_hh · h_{t-1} + b_h)    # 隐藏状态           │
│                                                                         │
│   y_t = W_hy · h_t + b_y                          # 输出               │
│                                                                         │
│   其中：                                                                │
│   • x_t: t时刻的输入向量                                                │
│   • h_t: t时刻的隐藏状态（记忆）                                        │
│   • h_{t-1}: t-1时刻的记忆                                             │
│   • W_xh: 输入到隐藏的权重                                              │
│   • W_hh: 隐藏到隐藏的权重（循环权重）                                  │
│   • W_hy: 隐藏到输出的权重                                              │
│   • tanh: 激活函数，将值压缩到[-1,1]                                   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">LSTM 解决长依赖问题</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   标准RNN问题：长序列时梯度会逐渐消失或爆炸                             │
│                                                                         │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │  LSTM 核心：三个"门"控制信息流动                                  │  │
│   │                                                                 │  │
│   │                     细胞状态 C_t                                 │  │
│   │  ┌──────────────────────────────────────────────────────────┐ │  │
│   │  │══════════════════════════════════════════════════════════│ │  │
│   │  └─────────────────────┬────────────────────────────────────┘ │  │
│   │                        │                                        │  │
│   │              ┌─────────┴─────────┐                            │  │
│   │              │     遗忘门        │   决定丢弃哪些信息          │  │
│   │              │  f_t = σ(W_f·[h_{t-1},x_t])                    │  │
│   │              └─────────┬─────────┘                            │  │
│   │                        │                                        │  │
│   │              ┌─────────┴─────────┐                            │  │
│   │              │     输入门        │   决定添加哪些新信息        │  │
│   │              │  i_t = σ(W_i·[h_{t-1},x_t])                    │  │
│   │              │  C̃_t = tanh(W_C·[h_{t-1},x_t])                │  │
│   │              └─────────┬─────────┘                            │  │
│   │                        │                                        │  │
│   │              ┌─────────┴─────────┐                            │  │
│   │              │     输出门        │   决定输出什么              │  │
│   │              │  o_t = σ(W_o·[h_{t-1},x_t])                    │  │
│   │              └─────────┬─────────┘                            │  │
│   │                        │                                        │  │
│   │                    h_t                                        │  │
│   └────────────────────────────────────────────────────────────────┘  │
│                                                                         │
│   ✓ 遗忘门：丢弃不需要的记忆                                            │
│   ✓ 输入门：添加新的重要信息                                           │
│   ✓ 输出门：决定当前输出什么                                          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">RNN vs LSTM vs GRU</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   普通RNN              LSTM                    GRU                       │
│   ┌─────┐            ┌─────┐                ┌─────┐                    │
│   │  h  │            │  C  │                │  h  │                    │
│   └─────┘            │  h  │                └─────┘                    │
│      │               └─────┘                   │                        │
│      │                  │                       │                        │
│   简单但               复杂但                   简化版                   │
│   梯度消失             效果好                   效果好                   │
│                      (3个门)                  (2个门)                   │
│                                                                         │
│   应用场景：                                                            │
│   • 文本生成：基于上文生成下一个词                                      │
│   • 机器翻译：编码源语言，解码目标语言                                  │
│   • 语音识别：处理音频序列                                              │
│   • 时间预测：股票、天气等时序数据                                      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="info-box warning">
                        <div class="info-box-title">⚠️ RNN的局限性</div>
                        <p>• <strong>梯度消失</strong>：长序列信息难以传递</p>
                        <p>• <strong>无法并行</strong>：必须按顺序计算，速度慢</p>
                        <p>• <strong>Transformer</strong> 解决了这些问题，已成为主流</p>
                    </div>
                </div>
            </div>

            <!-- Transformer 详解 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">⚡ Transformer 自注意力网络</h3>

            <div class="card">
                <div class="card-header">
                    <div class="card-icon">⚡</div>
                    <div>
                        <div class="card-title">Transformer - 当前最主流架构</div>
                        <div class="card-subtitle">GPT、BERT、Llama 的基础</div>
                    </div>
                </div>
                <div class="card-body">
                    <div class="diagram-box">
                        <div class="diagram-title">Transformer 整体架构</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│                           Transformer 编码器-解码器                          │
│                                                                             │
│   ┌─────────────────────┐         ┌─────────────────────┐                 │
│   │     输入嵌入        │         │     输出嵌入        │                 │
│   │   (Input Embed)     │         │  (Output Embed)     │                 │
│   └──────────┬──────────┘         └──────────┬──────────┘                 │
│              │                                │                            │
│              ▼                                ▼                            │
│   ┌─────────────────────┐         ┌─────────────────────┐                 │
│   │  + 位置编码         │         │  + 位置编码         │                 │
│   │ (Positional Encod)  │         │ (Positional Encod)  │                 │
│   └──────────┬──────────┘         └──────────┬──────────┘                 │
│              │                                │                            │
│              ▼                                │                            │
│   ┌─────────────────────┐                     │                            │
│   │   N× Encoder层      │                     │                            │
│   │  ┌───────────────┐  │                     │                            │
│   │  │ Self-Attention│  │                     │                            │
│   │  └───────────────┘  │                     │                            │
│   │  ┌───────────────┐  │                     │                            │
│   │  │  Feed-Forward │  │                     │                            │
│   │  └───────────────┘  │                     │                            │
│   └──────────┬──────────┘                     │                            │
│              │                                │                            │
│              │         ┌─────────────────────┤                            │
│              │         │                     │                            │
│              ▼         │                     ▼                            │
│   ┌─────────────────────┐         ┌─────────────────────┐                 │
│   │   N× Decoder层      │         │  线性层 + Softmax   │                 │
│   │  ┌───────────────┐  │         │  (Output Prob)     │                 │
│   │  │ Masked Attn   │  │         └─────────────────────┘                 │
│   │  └───────────────┘  │                                                     │
│   │  ┌───────────────┐  │                                                    │
│   │  │ Cross-Attn    │  │                                                    │
│   │  └───────────────┘  │                                                    │
│   │  ┌───────────────┐  │                                                    │
│   │  │ Feed-Forward  │  │                                                    │
│   │  └───────────────┘  │                                                    │
│   └──────────┬──────────┘                                                    │
│              │                                                               │
│              ▼                                                               │
│   ┌─────────────────────┐                                                    │
│   │   输出概率分布      │                                                    │
│   └─────────────────────┘                                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">自注意力机制 (Self-Attention) 核心原理</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   问题：如何让序列中每个位置都能"关注"到其他所有位置？                  │
│                                                                         │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │  Q (Query): 我想查询什么                                        │  │
│   │  K (Key): 我有什么特征                                          │  │
│   │  V (Value): 我的实际内容                                        │  │
│   │                                                                 │  │
│   │  Attention(Q, K, V) = softmax(Q·K^T/√d_k) · V                  │  │
│   └─────────────────────────────────────────────────────────────────┘  │
│                                                                         │
│   通俗理解：                                                            │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │                                                                 │  │
│   │   Query = 我想找"苹果"                                          │  │
│   │                                                                 │  │
│   │   Keys    | 相似度                                              │  │
│   │   ────────┼─────────────────────────────────────────           │  │
│   │   "苹果"  │  ★★★★★ (相似度=0.9)                               │  │
│   │   "香蕉"  │  ★★☆☆☆ (相似度=0.2)                               │  │
│   │   "橘子"  │  ★★★☆☆ (相似度=0.5)                               │  │
│   │                                                                 │  │
│   │   Value  × 相似度权重 = 最终关注的内容                           │  │
│   │                                                                 │  │
│   └─────────────────────────────────────────────────────────────────┘  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">多头注意力 (Multi-Head Attention)</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   多个注意力头并行处理，捕捉不同类型的关系                              │
│                                                                         │
│                         输入向量 X                                       │
│                            │                                            │
│                            ▼                                            │
│              ┌────────────┼────────────┐                               │
│              │            │            │                               │
│              ▼            ▼            ▼                               │
│           Head 1      Head 2      Head 3                               │
│           (位置)      (语义)      (语法)                               │
│              │            │            │                               │
│              └────────────┼────────────┘                               │
│                           │                                             │
│                           ▼                                             │
│                    拼接后线性变换                                        │
│                           │                                             │
│                           ▼                                             │
│                      输出向量                                           │
│                                                                         │
│   示例：句子 "苹果很好吃，我喜欢吃"                                    │
│   • Head 1: 关注"苹果"和"水果"的关系                                   │
│   • Head 2: 关注"苹果"和"好吃"的情感关系                               │
│   • Head 3: 关注"苹果"和"喜欢"的语法连接                               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">Transformer vs RNN 对比</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   ┌────────────────────────┬──────────────────────────────────────────┐ │
│   │          RNN           │            Transformer                   │ │
│   ├────────────────────────┼──────────────────────────────────────────┤ │
│   │  顺序处理，依赖上一步  │  并行处理，同时关注所有位置              │ │
│   │  O(n) 序列化          │  O(1) 并行化                             │ │
│   │  长序列梯度消失       │  自注意力无长依赖问题                    │ │
│   │  信息只能单向传递     │  双向/全局信息流动                       │ │
│   │  适合短序列           │  适合各种长度序列                        │ │
│   └────────────────────────┴──────────────────────────────────────────┘ │
│                                                                         │
│   为什么Transformer成为主流？                                           │
│   ✓ 可以并行训练，速度快                                                │
│   ✓ 捕捉长距离依赖                                                    │
│   ✓ 可扩展性强（GPT可达万亿参数）                                      │
│   ✓ 预训练效果好                                                      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="diagram-box">
                        <div class="diagram-title">基于Transformer的著名模型</div>
                        <div class="diagram-content">
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │                     BERT (2018)                                  │  │
│   │   双向Transformer编码器                                           │  │
│   │   预训练：掩码语言建模 + 下一句预测                               │  │
│   │   用途：分类、问答、命名实体识别                                 │  │
│   └─────────────────────────────────────────────────────────────────┘  │
│                                                                         │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │                     GPT系列 (2018-2024)                          │  │
│   │   单向Transformer解码器                                           │  │
│   │   GPT-2: 15亿参数 / GPT-3: 1750亿 / GPT-4: 万亿级               │  │
│   │   用途：文本生成、代码编写、对话                                 │  │
│   └─────────────────────────────────────────────────────────────────┘  │
│                                                                         │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │                     LLaMA / ChatGLM / Qwen (2023-)              │  │
│   │   开源大语言模型                                                  │  │
│   │   7B, 13B, 70B 参数版本                                          │  │
│   │   用途：本地部署、私有化部署                                     │  │
│   └─────────────────────────────────────────────────────────────────┘  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                        </div>
                    </div>

                    <div class="info-box tip">
                        <div class="info-box-title">💡 Transformer核心要点</div>
                        <p>✓ <strong>自注意力</strong>：每个位置都能直接关注其他所有位置</p>
                        <p>✓ <strong>多头</strong>：多个头捕捉不同类型的关系（语义、语法、位置）</p>
                        <p>✓ <strong>位置编码</strong>：用数学方式注入位置信息（因为没有循环结构）</p>
                        <p>✓ <strong>残差连接</strong>：帮助深层网络训练</p>
                        <p>✓ <strong>层归一化</strong>：稳定训练过程</p>
                    </div>
                </div>
            </div>

            <!-- 三种架构对比 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">📊 CNN vs RNN vs Transformer 对比</h3>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>特性</th>
                            <th>CNN</th>
                            <th>RNN/LSTM</th>
                            <th>Transformer</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>擅长数据</strong></td>
                            <td>图像、空间数据</td>
                            <td>序列、时序数据</td>
                            <td>任意序列</td>
                        </tr>
                        <tr>
                            <td><strong>核心机制</strong></td>
                            <td>卷积、池化</td>
                            <td>循环、门控</td>
                            <td>自注意力</td>
                        </tr>
                        <tr>
                            <td><strong>并行计算</strong></td>
                            <td>✓ 支持</td>
                            <td>✗ 顺序依赖</td>
                            <td>✓ 支持</td>
                        </tr>
                        <tr>
                            <td><strong>长依赖</strong></td>
                            <td>需要深网络</td>
                            <td>LSTM可缓解</td>
                            <td>✓ 天然支持</td>
                        </tr>
                        <tr>
                            <td><strong>参数共享</strong></td>
                            <td>卷积核共享</td>
                            <td>循环权重共享</td>
                            <td>位置无关</td>
                        </tr>
                        <tr>
                            <td><strong>典型应用</strong></td>
                            <td>图像分类、检测</td>
                            <td>机器翻译、语音</td>
                            <td>NLP、大语言模型</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- 核心概念 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">📚 核心概念</h3>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>概念</th>
                            <th>说明</th>
                            <th>简单理解</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>前向传播</strong></td>
                            <td>数据从输入层传到输出层</td>
                            <td>做计算题</td>
                        </tr>
                        <tr>
                            <td><strong>反向传播</strong></td>
                            <td>从后往前调整参数</td>
                            <td>对答案改错</td>
                        </tr>
                        <tr>
                            <td><strong>损失函数</strong></td>
                            <td>衡量预测和真实的差距</td>
                            <td>离正确答案有多远</td>
                        </tr>
                        <tr>
                            <td><strong>优化器</strong></td>
                            <td>调整参数减少损失</td>
                            <td>找到更好的方法</td>
                        </tr>
                        <tr>
                            <td><strong>学习率</strong></td>
                            <td>每次调整的步长</td>
                            <td>改多大程度的错</td>
                        </tr>
                        <tr>
                            <td><strong>Batch</strong></td>
                            <td>每次训练的数据量</td>
                            <td>一次看多少题</td>
                        </tr>
                        <tr>
                            <td><strong>Epoch</strong></td>
                            <td>完整遍历一次数据集</td>
                            <td>完整复习一遍</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- 训练技巧 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">⚙️ 训练技巧</h3>

            <div class="cards-grid">
                <div class="card">
                    <h4 style="margin-bottom: 1rem;">⚠️ 过拟合</h4>
                    <p style="font-size: 0.9rem; color: #666;">模型记住训练数据，不会泛化</p>
                    <div class="info-box warning" style="margin-top: 1rem;">
                        <p><strong>表现：</strong>训练集准确率很高，测试集准确率很低</p>
                        <p><strong>解决方法：</strong></p>
                        <ul style="margin-left: 1rem; font-size: 0.85rem;">
                            <li>增加训练数据</li>
                            <li>正则化（L1/L2）</li>
                            <li>Dropout随机丢弃</li>
                            <li>减少模型复杂度</li>
                        </ul>
                    </div>
                </div>

                <div class="card">
                    <h4 style="margin-bottom: 1rem;">⚠️ 欠拟合</h4>
                    <p style="font-size: 0.9rem; color: #666;">模型太简单，学不会</p>
                    <div class="info-box warning" style="margin-top: 1rem;">
                        <p><strong>表现：</strong>训练集和测试集准确率都很低</p>
                        <p><strong>解决方法：</strong></p>
                        <ul style="margin-left: 1rem; font-size: 0.85rem;">
                            <li>增加模型复杂度</li>
                            <li>增加训练时间</li>
                            <li>减少正则化</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 主流框架 -->
            <h3 style="margin: 2rem 0 1rem; font-size: 1.3rem;">🛠️ 主流深度学习框架</h3>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>框架</th>
                            <th>特点</th>
                            <th>适用场景</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>PyTorch</strong></td>
                            <td>Python风格，动态图，研究首选</td>
                            <td>学术研究、快速原型</td>
                        </tr>
                        <tr>
                            <td><strong>TensorFlow</strong></td>
                            <td>工业成熟，部署方便</td>
                            <td>生产部署、大规模训练</td>
                        </tr>
                        <tr>
                            <td><strong>Keras</strong></td>
                            <td>简单易用，上手快</td>
                            <td>入门学习、快速实验</td>
                        </tr>
                    </tbody>
                </table>
            </div>

        </div>
    </section>
    <!-- Prism.js syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-go.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
</body>
</html>